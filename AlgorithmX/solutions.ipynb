{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm X application to constituency data\n",
    "\n",
    "Previously we found all sets of 2 / 3 / 4 constituencies which are neighbours, i.e. those constituencies which share a border, which we shall call sets (with a unique identifier `set_no`). We will now apply Algorithm X to these merged constituencies and find (a subset of) solutions so that every constituency is selected once and only once. We shall do this on a region-by-region basis for two reasons:\n",
    "\n",
    "1. it will reduce the amount of possible combinations substantially\n",
    "1. it also (mostly) ensures consistency of political parties, so that e.g. we wouldn't have one constituency on England and one in Wales, so that Plaid Cymru vote would potentially halve.\n",
    "\n",
    "There are often times when the total number of constituencies in a region is not divisible by 2 / 3 / 4. For these cases we shall remove a set from a different constituency size until they are divisible, e.g. for the North East we have 29 constituencies so if we want to find all solutions where we merge 2 constituencies we shall pick at random one of the sets where 3 constituencies have been merged and remove them from our initial analysis. We shall repeat this, removing another of the 3-way merged sets, until we get a large enough sample.\n",
    "\n",
    "For some of the sets we have a large number of solutions, so we will only keep a subset of them. When there are a large number of solutions we shall rerun the analysis with the dataframe resampled and this can change the initial solutions given.\n",
    "\n",
    "The (sampled) solutions will be saved as csv files.\n",
    "\n",
    "All functions used are stored in the `algox_modules.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from AlgorithmX import *\n",
    "from joblib import Parallel, delayed\n",
    "from random import random, sample\n",
    "from algox_modules import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_pairs = pd.read_csv(\"../Analysis/Data/const_pairs.csv.gz\")\n",
    "const_tris = pd.read_csv(\"../Analysis/Data/const_tris.csv.gz\")\n",
    "const_quads = pd.read_csv(\"../Analysis/Data/const_quads.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.unique(const_pairs['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘Solutions/solns_*.csv.gz’: No such file or directory\n",
      "rm: cannot remove ‘Logs/check/solns_*.csv’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Remove any files that were created in a previous run\n",
    "!rm Logs/solns/soln_*.csv\n",
    "!rm Logs/log_*.log\n",
    "!rm Logs/DataFrames/df_*.csv.gz\n",
    "!rm Solutions/solns_*.csv.gz\n",
    "!rm Logs/check/solns_*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 35.2min\n"
     ]
    }
   ],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=2.5e5) \n",
    "        for region in regions for seats in [2,3,4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solns(const_pairs, const_tris, const_quads, 3, \"East\", max_solns=5e5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "for region in regions:\n",
    "    for seats in [2,3,4]:\n",
    "        print(f\"Region {region} with {seats} seats. Start time {datetime.datetime.now()}\")\n",
    "        get_solns(const_pairs, const_tris, const_quads, seats, region, max_solns=5e5) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=5e5) \n",
    "        for region in regions for seats in [2,3,4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['algox_modules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=2.5e5) \n",
    "        for region in regions for seats in [2,3,4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "soln_dict = {}\n",
    "for i in range(15):\n",
    "    files = glob.glob(f\"Logs/solns/soln_East_3_d_{i}.csv\")\n",
    "    if len(files) == 1:\n",
    "        soln_dict[i] = pd.read_csv(files[0])\n",
    "    else:\n",
    "        files = glob.glob(f\"Logs/solns/soln_East_3_d_{i}_*.csv\")\n",
    "        d = {}\n",
    "        for file in files:\n",
    "            j = int(file.replace(\".csv\", \"\").replace(\"Logs/solns/soln_East_3_d_\", \"\").split(\"_\")[1])\n",
    "            d[j] = pd.read_csv(file)\n",
    "        try:\n",
    "            soln_dict[i] = pd.concat(d, ignore_index=True)\n",
    "        except:\n",
    "            print(f\"For i = {i} cannot concatenate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(soln_dict)):\n",
    "    print(soln_dict[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_dict[6].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_dict[1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solns = pd.concat(soln_dict) #, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Logs/solns/soln_East_3_d_10_11.csv\"\n",
    "int(file.replace(\".csv\", \"\").replace(\"Logs/solns/soln_East_3_d_\", \"\").split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    r = region.replace(\" \", \"_\")\n",
    "    try:\n",
    "#         test2 = pd.read_csv(f\"Solutions/solns_{r}_2.csv.gz\")\n",
    "#         test3 = pd.read_csv(f\"Solutions/solns_{r}_3.csv.gz\")\n",
    "        test4 = pd.read_csv(f\"Solutions/solns_{r}_4.csv.gz\")\n",
    "        print(f\"We have {test4.shape[0]:,} solutions for the {region} region for 4 seats respectively.\")\n",
    "#         print(f\"We have {test2.shape[0]:,}, {test3.shape[0]:,} and {test4.shape[0]:,} solutions for the {region} region for 2, 3 and 4 seats respectively.\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solns(const_pairs, const_tris, const_quads, 4, \"East Midlands\", max_solns=5e5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"Solutions/solns_Scotland_3.csv.gz\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=5e5) \n",
    "        for region in regions for seats in [2,3,4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(f\"Logs/solns/soln_*_[0-9]_d_[0-9].csv\")\n",
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'West_Midlands'\n",
    "s = 3\n",
    "files = glob.glob(f\"Logs/solns/soln_{r}_{s}_d_[0-9].csv\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in ['Wales']: # regions:\n",
    "    r = region.replace(\" \", \"_\")\n",
    "    for s in [2, 3]:\n",
    "        files = glob.glob(f\"Logs/solns/soln_{r}_{s}_d_[0-9].csv\")\n",
    "        print(f\"{region}: {s} has {len(files)} files.\")\n",
    "        if len(files) > 0:\n",
    "            for file in files:\n",
    "                i = re.findall(\"[0-9]\", file)[1]\n",
    "                d[i] = pd.read_csv(file, converters={'soln': literal_eval})\n",
    "            test = pd.concat(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from ast import literal_eval\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    r = region.replace(\" \", \"_\")\n",
    "    for seats in [3]:\n",
    "        print(seats)\n",
    "        files = glob.glob(f\"Logs/solns/soln_{r}_{seats}_d_*.csv\")\n",
    "        d = {}\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            i = re.findall(\"[0-9]\", file)[1]\n",
    "            print(i)\n",
    "            d[i] = pd.read_csv(file, converters={'soln': literal_eval})\n",
    "#             print(d[i].shape)\n",
    "            \n",
    "            \n",
    "        test = pd.concat(d)\n",
    "#             print(file)\n",
    "#             print(re.findall(\"[0-9]\", file)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?re.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -f \"Logs/log_East_3.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_mapper(df):\n",
    "    \"\"\"\n",
    "    As the AlgorithmX code requires inputs starting from zero we shall take all values in the dataframes\n",
    "    and map them to ints. This function will return the solver required.\n",
    "    The df is always randomly resampled when we run this so that we get a different initial answer each time.\n",
    "    \"\"\"\n",
    "    name_cols = get_name_cols(df)\n",
    "    const_list = np.unique(df[name_cols].stack())\n",
    "    n = len(const_list)\n",
    "    mapping = {}\n",
    "    for i in range(n):\n",
    "        mapping[const_list[i]] = i\n",
    "    for col in name_cols:\n",
    "        df = df.replace({col: mapping})\n",
    "    solver = AlgorithmX(n)\n",
    "    for index, row in df.iterrows():\n",
    "        solver.appendRow([r for r in row[name_cols]], row['set_no'])\n",
    "    return solver\n",
    "\n",
    "def return_solutions(df, max_soln = 1e7, resampled=False, log_df_name=None):\n",
    "    \"\"\"\n",
    "    This function returns the solutions from the AlgorithmX code.\n",
    "    prop - states what proportion of the solutions are returned (useful for when they get too big)\n",
    "    max_soln - maximum number of solutions to derive\n",
    "    resampled - is this solution being rerun\n",
    "    \"\"\"\n",
    "    max_returned = 2.5e6\n",
    "    \n",
    "    solver = const_mapper(df)\n",
    "    solns = 0\n",
    "    dict_solns = {}\n",
    "    try:\n",
    "        with timeout(90, exception=RuntimeError): \n",
    "            # Stop calculations if taking too long, either there is no solution or having difficulty finding first one\n",
    "            for solution in solver.solve():\n",
    "                dict_solns[solns] = solution\n",
    "                solns += 1\n",
    "                if solns == max_soln:\n",
    "                    resampled = True # As we will be rerunning this with a dataframe 'resampled' data frame\n",
    "                    break\n",
    "            soln_returned = solns > 0\n",
    "\n",
    "            # If the result is too big take a sample. If the solution is going to be resampled take a small proportion\n",
    "            # otherwise take a larger one\n",
    "            if soln_returned:\n",
    "                if not resampled and solns <= max_returned:\n",
    "                    sampled_solns = pd.DataFrame({'soln': dict_solns}).reset_index(drop=True)\n",
    "                else:\n",
    "                    if not resampled:\n",
    "                        keys = sample(list(dict_solns.keys()), max_returned)\n",
    "                    else:\n",
    "                        keys = sample(list(dict_solns.keys()), int(max_soln*0.0025))\n",
    "                    dict_solns2 = {}\n",
    "                    for k in keys:\n",
    "                        dict_solns2[k] = dict_solns[k]\n",
    "                    sampled_solns = pd.DataFrame({'soln': dict_solns2}).reset_index(drop=True)\n",
    "                # Sort out the solutions at this point to save time later.\n",
    "                sampled_solns = sampled_solns.assign(soln = [list(np.sort(s)) for s in sampled_solns['soln']])\n",
    "                return soln_returned, sampled_solns, resampled\n",
    "            else:\n",
    "                soln_returned = False\n",
    "                return soln_returned, None, None\n",
    "    except RuntimeError:\n",
    "        soln_returned = False\n",
    "        return soln_returned, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "East\n",
    "East Midlands\n",
    "London\n",
    "North West\n",
    "South East\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'London'\n",
    "r = region.replace(\" \", \"_\")\n",
    "df = pd.read_csv(f\"Logs/DataFrames/df_{r}_3.csv.gz\")\n",
    "solver = const_mapper(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const_pairs2 = const_pairs.query(\"region == @region\")\n",
    "const_tris2 = const_tris.query(\"region == @region\")\n",
    "const_quads2 = const_quads.query(\"region == @region\")\n",
    "name_cols = get_name_cols(const_tris2)\n",
    "# How many times should we rerun Algorithm X when we cannot return all solutions.\n",
    "RERUN_COUNTER = 5 #* (1 + (seats >= 4))\n",
    "# How many times should we rerun Algorithm X when we have to remove different sized sets.\n",
    "COUNTER = 5 #* (1 + (seats >= 4))\n",
    "\n",
    "seats = 3\n",
    "n = get_n(df, name_cols)\n",
    "\n",
    "file_name = f\"Solutions/solns_{r}_{seats}.csv.gz\"\n",
    "log_file_name = f\"Logs/log_{r}_{seats}.log\"\n",
    "log_df_name = f\"Logs/DataFrames/df_{r}_{seats}.csv.gz\"\n",
    "log = custom_logger(log_file_name)\n",
    "log.info(f'Starting code for region {region} with {seats} seats.')\n",
    "max_solns = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the solutions multiple times with different random elements removed.\n",
    "soln_dict = {}\n",
    "i = 0\n",
    "removed = {}\n",
    "removed['triplet'] = [123]\n",
    "while i < COUNTER:\n",
    "#     df, removed = remove_random_const(const_pairs2, const_tris2, const_quads2, seats, region, n)\n",
    "    soln_returned, soln_dict[i], resampled = return_solutions(df, resampled=False, max_soln=max_solns, log_df_name=log_df_name)\n",
    "    if soln_returned:\n",
    "        if resampled:\n",
    "            d = {}\n",
    "            d[0] = soln_dict[i].copy()\n",
    "            j = 1\n",
    "            while j < RERUN_COUNTER and soln_returned:\n",
    "                if soln_returned:\n",
    "                    j += 1\n",
    "                    soln_returned, d[j], resampled = return_solutions(df, resampled=True, max_soln=max_solns, log_df_name=log_df_name)\n",
    "                else:\n",
    "                    break\n",
    "            if soln_returned:\n",
    "                soln_dict[i] = pd.concat(d)\n",
    "    if soln_returned:\n",
    "        # Add in the set_no's that were removed from the solutions\n",
    "        soln_dict[i][list(removed.keys())[0]] = str(list(removed.values())[0])\n",
    "        i += 1\n",
    "        solns = pd.concat(soln_dict)\n",
    "if len(solns) > 0:\n",
    "    solns = solns.assign(region = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    print(region)\n",
    "    get_solns(const_pairs, const_tris, const_quads, 2, region, max_solns=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solns(const_pairs, const_tris, const_quads, 2, \"South East\", max_solns=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['algox_modules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=4, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=1e7) \n",
    "        for seats in [2,3,4] for region in regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solns(const_pairs, const_tris, const_quads, 3, 'Yorkshire and the Humber', max_solns=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [2,3,4]:\n",
    "    get_solns(const_pairs, const_tris, const_quads, i, 'London', max_solns=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solns(const_pairs, const_tris, const_quads, 3, 'Wales', max_solns=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
