{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm X application to constituency data\n",
    "\n",
    "Previously we found all sets of 2 / 3 / 4 constituencies which are neighbours, i.e. those constituencies which share a border, which we shall call sets (with a unique identifier `set_no`). We will now apply Algorithm X to these merged constituencies and find (a subset of) solutions so that every constituency is selected once and only once. We shall do this on a region-by-region basis for two reasons:\n",
    "\n",
    "1. it will reduce the amount of possible combinations substantially\n",
    "1. it also (mostly) ensures consistency of political parties, so that e.g. we wouldn't have one constituency on England and one in Wales, so that Plaid Cymru vote would potentially halve.\n",
    "\n",
    "There are often times when the total number of constituencies in a region is not divisible by 2 / 3 / 4. For these cases we shall remove a set from a different constituency size until they are divisible, e.g. for the North East we have 29 constituencies so if we want to find all solutions where we merge 2 constituencies we shall pick at random one of the sets where 3 constituencies have been merged and remove them from our initial analysis. We shall repeat this, removing another of the 3-way merged sets, until we get a large enough sample.\n",
    "\n",
    "For some of the sets we have a large number of solutions, so we will only keep a subset of them. When there are a large number of solutions we shall rerun the analysis with the dataframe resampled and this can change the initial solutions given.\n",
    "\n",
    "The (sampled) solutions will be saved as csv files.\n",
    "\n",
    "All functions used are stored in the `algox_modules.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from algox_modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_pairs = pd.read_csv(\"../Analysis/Data/const_pairs.csv.gz\")\n",
    "const_tris = pd.read_csv(\"../Analysis/Data/const_tris.csv.gz\")\n",
    "const_quads = pd.read_csv(\"../Analysis/Data/const_quads.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.unique(const_pairs['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders used to store logs and info during runthrough\n",
    "import os\n",
    "if not os.path.isdir(\"Logs\"):\n",
    "    os.makedirs(\"Logs/\")\n",
    "    \n",
    "# Remove any files that were created in a previous run\n",
    "import glob\n",
    "def del_files(dir):\n",
    "    files = glob.glob(dir)\n",
    "    if len(files) > 0:\n",
    "        [os.remove(f) for f in files]\n",
    "del_files(\"Logs/log_*.log\")\n",
    "del_files(\"Solutions/solns_*.csv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed: 56.1min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed: 179.7min\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 782.2min\n",
      "[Parallel(n_jobs=5)]: Done  31 out of  36 | elapsed: 1505.9min remaining: 242.9min\n",
      "[Parallel(n_jobs=5)]: Done  36 out of  36 | elapsed: 4486.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Command to run with joblib.\n",
    "element_information = Parallel(n_jobs=5, verbose=10)(\n",
    "    delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=1e5) \n",
    "        for region in regions for seats in [2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code took about 3 days to run. It might have been advantageous to have run the longest running jobs first\n",
    "# So the above code could have been written as\n",
    "# regions = [\"London\",\"South East\",\"West Midlands\",\"North West\",\"Scotland\",\"East\",\"Yorkshire and The Humber\",\"South West\",\"East Midlands\",\"Wales\",\"North East\",\"Northern Ireland\"]\n",
    "# element_information = Parallel(n_jobs=5, verbose=10)(\n",
    "#     delayed(get_solns)(const_pairs, const_tris, const_quads, seats, region, max_solns=1e5) \n",
    "#         for seats in [4,3,2]) for region in regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to remove any duplicates that may have occured.\n",
    "# In addition only keep a maximum of 25,000 solutions\n",
    "import glob\n",
    "from ast import literal_eval\n",
    "sampled_solutions = 25000\n",
    "if not os.path.isdir(\"../Analysis/Data/SampledSolutions/\"):\n",
    "    os.makedirs(\"../Analysis/Data/SampledSolutions/\")\n",
    "files = glob.glob(\"Solutions/solns_*.csv.gz\")\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, dtype={'region': str}, converters={'soln': literal_eval})\n",
    "    df2 = pd.DataFrame(df['soln'].tolist())\n",
    "    df3 = df2.drop_duplicates()\n",
    "    if df2.shape[0] != df3.shape[0]:\n",
    "        df = df[df.index.isin(df3.index)].reset_index(drop=True)\n",
    "    # Save a sample of 'sampled_solutions' (if the number of solutions is bigger than that)\n",
    "    file_name = file.replace(\"Solutions/solns_\", \"../Analysis/Data/SampledSolutions/sampled_solns_\")\n",
    "    if df.shape[0] > sampled_solutions:\n",
    "        df.sample(sampled_solutions).to_csv(file_name, index=False)\n",
    "    else:\n",
    "        df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
